{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cf94576-cfcd-4d9a-a6ff-8f1153f8b65d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#q10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "735d980f-560a-432c-8ed9-7f794247eed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "The Eigen-Decomposition approach is a powerful tool in data analysis and machine learning, providing several applications and techniques that rely on it. Here are three specific applications or techniques:\n",
    "\n",
    "1. Principal Component Analysis (PCA): PCA is a widely used technique in data analysis and machine learning that relies on Eigen-Decomposition to transform the original dataset into a new set of variables or principal components. By doing so, PCA identifies the directions of maximum variation in the dataset, with the first principal component accounting for the largest amount of variance in the data, the second principal component accounting for the second-largest amount of variance, and so on. PCA is useful for dimensionality reduction, data visualization, feature selection, and data compression, among other applications.\n",
    "\n",
    "2. Singular Value Decomposition (SVD): SVD is another widely used technique in data analysis and machine learning that relies on Eigen-Decomposition. It factorizes a matrix into three matrices: the left-singular vectors, the right-singular vectors, and the singular values. SVD has several applications, such as data compression, image processing, natural language processing, collaborative filtering, and latent factor models.\n",
    "\n",
    "3. Eigenfaces: Eigenfaces is an application of Eigen-Decomposition in facial recognition, where the goal is to recognize a person's face from a database of images. Eigenfaces uses PCA to transform the face images into a lower-dimensional space, where each image is represented as a linear combination of Eigenfaces or principal components. The principal components are obtained through Eigen-Decomposition of the covariance matrix of the face images. Eigenfaces have several applications in security systems, human-computer interaction, and personal identification.\n",
    "\n",
    "In summary, Eigen-Decomposition is a useful tool in data analysis and machine learning, providing several applications and techniques, such as PCA, SVD, and Eigenfaces. These techniques rely on Eigen-Decomposition to transform the data into a lower-dimensional space, to identify the directions of maximum variation, and to extract meaningful information from the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22406a54-0a5b-400b-865a-a0d270c6c8e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#q9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50187467-5c14-41ee-abc2-fb5046bedfbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "A matrix can have more than one set of eigenvectors and eigenvalues under certain conditions. \n",
    "\n",
    "First, it's important to note that eigenvectors and eigenvalues are always found in pairs. For a square matrix A, an eigenvector v is a non-zero vector that, when multiplied by A, results in a scalar multiple of itself, or Av = λv, where λ is the corresponding eigenvalue.\n",
    "\n",
    "If a matrix A is not diagonalizable, meaning that it cannot be transformed into a diagonal matrix through similarity transformations, then it may have repeated eigenvalues and/or linearly dependent eigenvectors. In this case, there can be multiple sets of eigenvectors and eigenvalues associated with the matrix A.\n",
    "\n",
    "For example, consider the following matrix:\n",
    "\n",
    "A = [1 1; 0 1]\n",
    "\n",
    "The eigenvalues of A can be found by solving the characteristic equation:\n",
    "\n",
    "det(A - λI) = 0\n",
    "\n",
    "which yields:\n",
    "\n",
    "(1 - λ)^2 = 0\n",
    "\n",
    "λ = 1 (with multiplicity 2)\n",
    "\n",
    "So, A has only one eigenvalue, 1, with multiplicity 2. To find the eigenvectors associated with this eigenvalue, we solve the equation:\n",
    "\n",
    "(A - λI)v = 0\n",
    "\n",
    "which yields:\n",
    "\n",
    "[0 1; 0 0]v = 0\n",
    "\n",
    "v = [1; 0]\n",
    "\n",
    "Since the matrix A has only one distinct eigenvalue, it has only one set of linearly independent eigenvectors associated with that eigenvalue. However, it is possible to find a set of two linearly dependent eigenvectors that span the same eigenspace, which is the space of all vectors that are mapped to scalar multiples of the same eigenvector.\n",
    "\n",
    "In summary, a matrix can have more than one set of eigenvectors and eigenvalues under certain conditions, such as when the matrix is not diagonalizable, or when it has repeated eigenvalues and/or linearly dependent eigenvectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cceac2c8-4a51-4d08-8ea5-260d66d1b78e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#q8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7db9845-3af3-4902-8c59-09d8404032af",
   "metadata": {},
   "outputs": [],
   "source": [
    "Eigen-Decomposition has several real-world applications across different fields. Here are a few examples:\n",
    "\n",
    "1. Image and Signal Processing: Eigen-Decomposition is used to analyze images and signals, such as audio and video data. For example, it is used in image compression techniques like JPEG and MPEG, where images are transformed into a lower-dimensional space using the Singular Value Decomposition (SVD) technique, which is based on Eigen-Decomposition. SVD can also be used to remove noise from images or signals by retaining only the most significant Eigenvalues and Eigenvectors.\n",
    "\n",
    "2. Recommender Systems: Eigen-Decomposition is used in collaborative filtering algorithms to build recommender systems. These algorithms analyze user-item interactions and create a matrix of ratings, which is then factorized using Eigen-Decomposition techniques like SVD or Alternating Least Squares (ALS) to find latent factors or Eigenvalues and Eigenvectors that represent user and item preferences. This allows the system to make personalized recommendations to users based on their preferences.\n",
    "\n",
    "3. Quantum Mechanics: Eigen-Decomposition plays a crucial role in quantum mechanics, where it is used to find the Eigenstates and Eigenvalues of quantum mechanical systems. These Eigenstates represent the different possible states of the system, and the Eigenvalues represent the energies associated with those states. By finding the Eigenstates and Eigenvalues of a quantum mechanical system, physicists can predict the behavior of the system under different conditions and make accurate predictions.\n",
    "\n",
    "4. Structural Engineering: Eigen-Decomposition is used in structural engineering to analyze the behavior of structures under different loads and vibrations. By finding the Eigenvalues and Eigenvectors of a structure, engineers can identify its natural frequencies and modes of vibration, which can help them design structures that are resistant to earthquakes and other natural disasters.\n",
    "\n",
    "In summary, Eigen-Decomposition has several real-world applications across different fields, such as image and signal processing, recommender systems, quantum mechanics, and structural engineering. These applications rely on the ability of Eigen-Decomposition to transform complex data into a more meaningful and interpretable form, allowing analysts and scientists to gain valuable insights and make accurate predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "407f8002-2e8a-4a23-9bff-6034651d25a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#q7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf3c29f2-ca27-439c-a797-8c70ab2f2d16",
   "metadata": {},
   "outputs": [],
   "source": [
    "Yes, the geometric interpretation of eigenvectors and eigenvalues is an important aspect of linear algebra.\n",
    "\n",
    "Eigenvectors are vectors that do not change direction when a linear transformation is applied to them. Instead, they are only scaled by a scalar value, which is known as the eigenvalue. The eigenvalue represents the amount of scaling that occurs along the eigenvector during the transformation.\n",
    "\n",
    "Geometrically, eigenvectors can be thought of as the axes of an ellipsoid that is transformed by the linear transformation. The eigenvalues represent the lengths of the principal axes of this ellipsoid.\n",
    "\n",
    "For example, consider the following 2x2 matrix:\n",
    "\n",
    "A = [2 1; 1 2]\n",
    "\n",
    "The eigenvectors and eigenvalues of A can be found by solving the equation:\n",
    "\n",
    "A v = λ v\n",
    "\n",
    "where v is the eigenvector and λ is the eigenvalue.\n",
    "\n",
    "Solving this equation yields the following eigenvectors and eigenvalues:\n",
    "\n",
    "λ1 = 3, v1 = [1, 1]\n",
    "\n",
    "λ2 = 1, v2 = [-1, 1]\n",
    "\n",
    "Geometrically, this means that when A is applied to the eigenvector v1 = [1, 1], it only scales the vector by a factor of 3, while maintaining its direction. This eigenvector is aligned with the principal diagonal of the ellipsoid that is transformed by A, and its eigenvalue, 3, represents the length of this diagonal.\n",
    "\n",
    "Similarly, when A is applied to the eigenvector v2 = [-1, 1], it only scales the vector by a factor of 1, while maintaining its direction. This eigenvector is aligned with the secondary diagonal of the ellipsoid that is transformed by A, and its eigenvalue, 1, represents the length of this diagonal.\n",
    "\n",
    "In general, eigenvectors represent the directions in which a linear transformation only stretches or compresses the vector, while eigenvalues represent the amount of stretching or compressing along those directions.\n",
    "\n",
    "In summary, the geometric interpretation of eigenvectors and eigenvalues helps us understand the behavior of linear transformations and their effect on the geometry of the data. It provides a powerful tool for visualizing and analyzing high-dimensional data and is widely used in machine learning, computer vision, and other fields."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e69194c7-4f23-4575-9a36-e5983dbca4ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "#q6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe6314c2-f600-4d47-9407-1c27287df3e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "Eigenvectors are a special set of vectors that are associated with linear transformations of a vector space. When a linear transformation is applied to an eigenvector, the eigenvector is scaled by a scalar value, which is known as the eigenvalue. In other words, the eigenvector is only transformed by a scalar multiple of itself.\n",
    "\n",
    "More formally, given a linear transformation T, an eigenvector x and an associated scalar value λ, we have:\n",
    "\n",
    "T(x) = λx\n",
    "\n",
    "where x is the eigenvector and λ is the corresponding eigenvalue.\n",
    "\n",
    "Eigenvectors are important because they provide a way to understand how a linear transformation changes a vector space. They represent directions in the vector space that are preserved or scaled by the linear transformation. For example, in a 2D vector space, an eigenvector may represent a line that is stretched or compressed by a linear transformation, while its corresponding eigenvalue represents the amount of stretching or compression.\n",
    "\n",
    "Eigenvalues and eigenvectors always occur in pairs, where each eigenvector is associated with a unique eigenvalue. The eigenvalue represents the scaling factor applied to the eigenvector when the linear transformation is applied. If the eigenvector is scaled by a positive factor, then the eigenvalue is positive. If the eigenvector is flipped by the linear transformation, then the eigenvalue is negative. And if the eigenvector is unchanged by the linear transformation, then the eigenvalue is zero.\n",
    "\n",
    "Eigenvectors and eigenvalues are widely used in linear algebra, machine learning, and data analysis. They provide a way to reduce the dimensionality of data, analyze patterns in high-dimensional data, and understand the behavior of linear transformations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0412a77f-0a8c-46c5-a4df-1a0d7630f578",
   "metadata": {},
   "outputs": [],
   "source": [
    "#q5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cea1ced-fa0f-4a20-ac89-6786f5dbdb12",
   "metadata": {},
   "outputs": [],
   "source": [
    "To find the eigenvalues of a matrix, you need to solve the characteristic equation of the matrix, which is given by:\n",
    "\n",
    "det(A - λI) = 0\n",
    "\n",
    "where A is the matrix, λ is the eigenvalue, and I is the identity matrix. The determinant of the matrix A - λI must be zero in order to find the eigenvalues.\n",
    "\n",
    "Once you have solved the characteristic equation, the resulting values of λ are the eigenvalues of the matrix A. Each eigenvalue corresponds to a unique eigenvector, which can be found by solving the equation:\n",
    "\n",
    "(A - λI)x = 0\n",
    "\n",
    "where x is the eigenvector.\n",
    "\n",
    "The eigenvalues represent the scaling factors that are applied to the eigenvectors when the matrix A is applied as a linear transformation. They are a measure of how much the eigenvectors are stretched or compressed by the linear transformation. The sign of the eigenvalue indicates whether the eigenvector is flipped or not flipped by the transformation. If the eigenvalue is positive, the eigenvector is stretched; if it is negative, the eigenvector is compressed; and if it is zero, the eigenvector is unchanged.\n",
    "\n",
    "Eigenvalues are important in many areas of mathematics and science, including linear algebra, differential equations, physics, and engineering. They are used to solve systems of linear equations, analyze dynamical systems, understand the behavior of quantum systems, and much more. In data analysis and machine learning, eigenvalues are used in techniques such as principal component analysis (PCA) to reduce the dimensionality of data and extract important features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca4ade4a-6a33-463c-b028-172d67042607",
   "metadata": {},
   "outputs": [],
   "source": [
    "#q4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "990a2e6b-9b09-402c-a214-7b5ed76deb72",
   "metadata": {},
   "outputs": [],
   "source": [
    "The spectral theorem is a fundamental result in linear algebra that relates the eigenvalues and eigenvectors of a symmetric matrix to its diagonalization. It states that any real symmetric matrix A can be diagonalized by an orthogonal matrix Q, which consists of the eigenvectors of A, as follows:\n",
    "\n",
    "A = QΛQ^T\n",
    "\n",
    "where Λ is a diagonal matrix containing the eigenvalues of A, and Q^T is the transpose of Q.\n",
    "\n",
    "The spectral theorem is significant in the context of the Eigen-Decomposition approach because it provides a way to decompose a symmetric matrix into its eigenvalues and eigenvectors, which can then be used to analyze and understand the matrix. For example, the spectral theorem can be used to find the principal components of a data set using PCA, by decomposing the covariance matrix of the data into its eigenvectors and eigenvalues.\n",
    "\n",
    "The diagonalizability of a matrix is closely related to the spectral theorem. A matrix is diagonalizable if it can be decomposed into a diagonal matrix D and an invertible matrix P such that P^-1AP = D. In other words, a matrix is diagonalizable if it can be transformed into a diagonal matrix by a change of basis.\n",
    "\n",
    "A matrix is diagonalizable if and only if it has n linearly independent eigenvectors, where n is the dimension of the matrix. In the case of a symmetric matrix, the eigenvectors are orthogonal to each other, and can be used to form an orthonormal basis for the vector space. This allows the matrix to be diagonalized by an orthogonal matrix, as given by the spectral theorem.\n",
    "\n",
    "For example, consider the symmetric matrix A given by:\n",
    "\n",
    "A = [[3, 2], [2, 5]]\n",
    "\n",
    "To diagonalize A, we first need to find its eigenvalues and eigenvectors. The characteristic equation of A is:\n",
    "\n",
    "det(A - λI) = 0\n",
    "= det([[3, 2], [2, 5]] - λ[[1, 0], [0, 1]])\n",
    "= det([[3-λ, 2], [2, 5-λ]])\n",
    "= (3-λ)(5-λ) - 4\n",
    "= λ^2 - 8λ + 11\n",
    "\n",
    "Solving this equation, we get the eigenvalues λ1 = 1 and λ2 = 7. To find the eigenvectors, we substitute each eigenvalue into the equation (A - λI)x = 0, and solve for x. This gives us the eigenvectors v1 = [-0.71, 0.71] and v2 = [-0.71, -0.71].\n",
    "\n",
    "We can verify that these eigenvectors are orthogonal and have unit length, which means they form an orthonormal basis for the vector space. Using these eigenvectors, we can form the orthogonal matrix Q as:\n",
    "\n",
    "Q = [[-0.71, 0.71], [-0.71, -0.71]]\n",
    "\n",
    "The diagonal matrix Λ is formed by placing the eigenvalues along the diagonal, as follows:\n",
    "\n",
    "Λ = [[1, 0], [0, 7]]\n",
    "\n",
    "Finally, we can use the spectral theorem to write A as:\n",
    "\n",
    "A = QΛQ^T\n",
    "= [[-0.71, 0.71], [-0.71, -0.71]] [[1, 0], [0, 7]] [[-0.71, -0.71], [0.71, -0.71]]\n",
    "\n",
    "This shows that A can be diagonalized by the orthogonal matrix Q, which consists of the eigenvectors of A, and the diagonal matrix Λ, which contains the eigenvalues of A."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05ca6872-6d85-4178-af32-983ccc83de30",
   "metadata": {},
   "outputs": [],
   "source": [
    "#q3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "429de0c7-0161-4412-9a13-dcd1ef2ba96e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "A square matrix $A$ is diagonalizable using the Eigen-Decomposition approach if and only if it has $n$ linearly independent eigenvectors, where $n$ is the size of the matrix. \n",
    "\n",
    "Proof:\n",
    "\n",
    "Suppose $A$ has $n$ linearly independent eigenvectors $\\mathbf{v}_1, \\mathbf{v}_2, \\ldots, \\mathbf{v}_n$ with corresponding eigenvalues $\\lambda_1, \\lambda_2, \\ldots, \\lambda_n$. Let $V$ be the matrix whose columns are the eigenvectors of $A$. Then, we can write:\n",
    "\n",
    "$$AV = V\\Lambda$$\n",
    "\n",
    "where $\\Lambda$ is the diagonal matrix whose diagonal entries are the eigenvalues of $A$. This can be rewritten as:\n",
    "\n",
    "$$A = V\\Lambda V^{-1}$$\n",
    "\n",
    "Since $V$ is invertible (since its columns are linearly independent), we can diagonalize $A$ as $A = V\\Lambda V^{-1}$.\n",
    "\n",
    "Conversely, suppose $A$ is diagonalizable, i.e., we can write $A = V\\Lambda V^{-1}$ for some invertible matrix $V$ and diagonal matrix $\\Lambda$. Let $\\mathbf{v}_1, \\mathbf{v}_2, \\ldots, \\mathbf{v}_n$ be the columns of $V$. Then, we have:\n",
    "\n",
    "$$AV = V\\Lambda$$\n",
    "\n",
    "which can be rewritten as:\n",
    "\n",
    "$$A\\mathbf{v}_i = \\lambda_i \\mathbf{v}_i$$\n",
    "\n",
    "Thus, $\\mathbf{v}_i$ is an eigenvector of $A$ with eigenvalue $\\lambda_i$. Since $V$ is invertible, its columns are linearly independent. Therefore, $A$ has $n$ linearly independent eigenvectors, as required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaae272e-003d-45e1-a93e-2d2963773401",
   "metadata": {},
   "outputs": [],
   "source": [
    "#q2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d95e008a-4f5e-4807-b710-b3c92f9435a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "Eigen decomposition is a process in linear algebra that decomposes a matrix into a set of eigenvectors and eigenvalues. It is also known as diagonalization because the resulting diagonal matrix of eigenvalues is often easier to work with than the original matrix.\n",
    "\n",
    "The significance of eigen decomposition lies in its ability to reveal important properties of a matrix, such as its symmetry, invertibility, and positive definiteness. Eigenvectors and eigenvalues are also used in a variety of mathematical applications, such as principal component analysis (PCA), signal processing, and differential equations.\n",
    "\n",
    "In addition, eigen decomposition can be used to solve systems of linear differential equations. In this case, the eigenvectors of the matrix correspond to the modes of the system and the eigenvalues correspond to the growth rates or decay rates of those modes.\n",
    "\n",
    "Overall, eigen decomposition is a powerful tool in linear algebra that has wide-ranging applications in various fields of mathematics and beyond."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bf4110d-e96f-44f4-a4f5-b2bde95ecaa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#q1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70fd94d8-f0f5-4927-91d4-b5b2b1803689",
   "metadata": {},
   "outputs": [],
   "source": [
    "Eigenvalues and eigenvectors are important concepts in linear algebra that are closely related to the eigen-decomposition approach. \n",
    "\n",
    "An eigenvector of a matrix A is a non-zero vector x such that when A is multiplied by x, the resulting vector is a scalar multiple of x, i.e., Ax = λx, where λ is a scalar called the eigenvalue corresponding to the eigenvector x.\n",
    "\n",
    "The eigen-decomposition approach decomposes a matrix A into a product of a matrix of eigenvectors and a diagonal matrix of corresponding eigenvalues. In other words, A = VΛV^-1, where V is a matrix whose columns are eigenvectors of A, and Λ is a diagonal matrix whose diagonal entries are the corresponding eigenvalues of A.\n",
    "\n",
    "As an example, let's consider the following 2x2 matrix:\n",
    "\n",
    "A = [3 1; 1 3]\n",
    "\n",
    "To find the eigenvectors and eigenvalues of A, we solve the equation Ax = λx, where x is a non-zero vector. \n",
    "\n",
    "Substituting A and x into the equation, we get:\n",
    "\n",
    "[3 1; 1 3][x1; x2] = λ[x1; x2]\n",
    "\n",
    "Expanding the matrix multiplication, we get two equations:\n",
    "\n",
    "3x1 + x2 = λx1\n",
    "x1 + 3x2 = λx2\n",
    "\n",
    "We can rearrange these equations to obtain:\n",
    "\n",
    "(3 - λ)x1 + x2 = 0\n",
    "x1 + (3 - λ)x2 = 0\n",
    "\n",
    "For non-trivial solutions (i.e., x ≠ 0), the determinant of the coefficient matrix must be 0. Therefore, we have:\n",
    "\n",
    "(3 - λ)(3 - λ) - 1 = 0\n",
    "\n",
    "Expanding and simplifying, we get:\n",
    "\n",
    "λ^2 - 6λ + 8 = 0\n",
    "\n",
    "Solving for λ, we get:\n",
    "\n",
    "λ1 = 2\n",
    "λ2 = 4\n",
    "\n",
    "Now we can find the corresponding eigenvectors by substituting each eigenvalue into the equations (3 - λ)x1 + x2 = 0 and x1 + (3 - λ)x2 = 0 and solving for x. \n",
    "\n",
    "For λ1 = 2, we get:\n",
    "\n",
    "x1 + x2 = 0\n",
    "\n",
    "which has a non-zero solution of x = [1; -1]. Therefore, the eigenvector corresponding to λ1 is [1; -1].\n",
    "\n",
    "For λ2 = 4, we get:\n",
    "\n",
    "x1 + x2 = 0\n",
    "\n",
    "which has a non-zero solution of x = [1; -1]. Therefore, the eigenvector corresponding to λ2 is also [1; -1].\n",
    "\n",
    "Now we can form the matrix V by placing the eigenvectors as columns:\n",
    "\n",
    "V = [1 1; -1 -1]\n",
    "\n",
    "And the diagonal matrix Λ with the eigenvalues on the diagonal:\n",
    "\n",
    "Λ = [2 0; 0 4]\n",
    "\n",
    "Therefore, we can write:\n",
    "\n",
    "A = VΛV^-1\n",
    "\n",
    "which gives us the eigen-decomposition of A."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aebb4ca2-a792-458d-b056-d2af46017193",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23dc3719-2d77-4978-a280-4651258d61c5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bdcaa1f-1917-45c6-93af-50ed3bbbd9f0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
